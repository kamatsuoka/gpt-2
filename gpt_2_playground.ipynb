{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gpt-2-playground.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kamatsuoka/gpt-2/blob/master/gpt_2_playground.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tI-HVDbQS9dF"
      },
      "source": [
        "# Text Generation Playground\n",
        "\n",
        "# Background\n",
        "In this notebook you can play around with generating text using the medium (345M parameter) version of [Open AI's GPT-2 model](https://openai.com/blog/better-language-models/).\n",
        "\n",
        "Briefly, GPT-2 is a kind neural network called a [transformer](https://www.tensorflow.org/alpha/tutorials/text/transformer), trained on millions of web documents shared in Reddit posts with a score of at least 3.  The model learns to predict the next word, given a sequence of words.  By repeating the predicition process, the model can generate full sentences and paragraphs.  The results are often quite interesting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lil_k1vVuEt-"
      },
      "source": [
        "##1. Install Code and Data\n",
        "Download the model data and install Python libraries.   This will take a minute.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLtLRI99TS2z",
        "colab_type": "code",
        "outputId": "08fbd983-ffff-4276-b0c8-c5ecb17b582c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import os\n",
        "import sys\n",
        "basedir = '/content' # specific to colaboratory\n",
        "os.chdir(basedir)\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "!rm -rf gpt-2\n",
        "!git clone -q https://github.com/kamatsuoka/gpt-2/\n",
        "os.chdir('gpt-2')\n",
        "sys.path.append(os.getcwd() + '/src')\n",
        "!pip3 --quiet install -r requirements-colab.txt\n",
        "!python download_model.py 345M --quiet\n",
        "from src.conditional_samples import restore_model, generate_samples\n",
        "sess, hparams, sequence_output, enc, placeholders = restore_model()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from models/345M/model.ckpt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ycov0bwftzie"
      },
      "source": [
        "## 2. Generate samples conditioned on starting text\n",
        "\n",
        "Enter starting text and optionally change the numeric parameters below.<br>\n",
        "Then run the cell to generate samples conditioned on the starting text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-8k0v3rTS24",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@markdown ### Starting text:\n",
        "starting_text = \"\" #@param {type:\"string\"}\n",
        "#@markdown ### Samples to generate:\n",
        "nsamples = 3 #@param {type:\"slider\", min:1, max:10, step:1}\n",
        "#@markdown ### Number of words per sample:\n",
        "length = 200 #@param {type:\"slider\", min:10, max:500, step:1}\n",
        "#@markdown ### Randomness in choosing the next word:\n",
        "temperature = 0.9 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "#@markdown ### Number of words to consider for next word:\n",
        "top_k = 40 #@param {type:\"slider\", min:0, max:200, step:1}\n",
        "\n",
        "from IPython.display import display, HTML\n",
        "from html import escape \n",
        "\n",
        "def escape_sample(sample):\n",
        "    return map(escape, sample.split('<|endoftext|>')[0].split(\"\\n\"))\n",
        "  \n",
        "def text_to_html(sample):\n",
        "    display(HTML(\"<p><i>\" + escape(starting_text) + \"</i>\" + \n",
        "         \"<br/>\".join(escape_sample(sample)) + \"</p><hr/>\"))\n",
        "  \n",
        "styles = \"\"\"\n",
        "  p { font-size: 150%; margin-top: 1em;  margin-bottom: 1em; }\n",
        "\"\"\"  \n",
        "\n",
        "display(HTML(\"<style>\" + styles + \"</style><h1>Samples</h1>\"))  \n",
        "generate_samples(\n",
        "        sess,\n",
        "        hparams,\n",
        "        sequence_output,\n",
        "        enc,\n",
        "        placeholders,\n",
        "        print_fn = text_to_html,\n",
        "        starting_text = starting_text,\n",
        "        nsamples=nsamples,\n",
        "        length=length,\n",
        "        temperature=temperature,\n",
        "        top_k=top_k)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}